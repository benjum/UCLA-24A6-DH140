


from sklearn.datasets import fetch_20newsgroups


data = fetch_20newsgroups(remove=("headers", "footers", "quotes"))


print(data.DESCR)


x = data.data


len(x)


x[0]


data.target_names


data.target





from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from string import punctuation

# getting corpora
import nltk
nltk.download('stopwords')
nltk.download('punkt')


myStopWords = list(punctuation) + stopwords.words('english')


x[0]


[w for w in word_tokenize(x[0].lower()) if w not in myStopWords]


docs = []
for i in x:
    docs.append([w for w in word_tokenize(i.lower()) if w not in myStopWords])


docs[0]


from nltk.stem.porter import PorterStemmer
#from nltk.stem import LancasterStemmer


# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()


docs_stemmed = []
for i in docs:
    docs_stemmed.append([p_stemmer.stem(w) for w in i])


docs_stemmed[0]





from gensim import corpora, models
import gensim


dictionary = corpora.Dictionary(docs_stemmed)


len(dictionary)


dictionary.filter_extremes(no_below=10, no_above=0.5)
# could also trim with keep_n=1000 or similar to keep only the top words


len(dictionary)


print(dictionary.token2id)


print(dictionary.token2id['patient'])


dictionary[1668]


corpus = [dictionary.doc2bow(text) for text in docs_stemmed]


print(corpus[30])


dictionary[276]


docs_stemmed[30]


wordid = corpus[30][0]
print(dictionary[wordid[0]],wordid[1])


for i in corpus[30]:
    print(dictionary[i[0]], i[1])


ldamodel = gensim.models.ldamodel.LdaModel(corpus, 
                                           num_topics=20, 
                                           id2word = dictionary, 
                                           passes=5)


ldamodel.show_topics(num_topics=20)


for i in ldamodel.print_topics(num_topics=20, num_words=20):
    print(i[0])
    print(i[1])
    print('\n')


data.target_names


import matplotlib.pyplot as plt
import re


re.split(re.escape(' + ') + '|' + re.escape('*'), 'hi + me*4')


fig,ax = plt.subplots(5,4,figsize=(15,20))
ax = ax.flatten()
for i in ldamodel.print_topics(num_topics=20, num_words=20):
    x = []
    y = []
    count = 0
    for j in re.split(re.escape(' + ') + '|' + re.escape('*'), i[1]):
        if count % 2 == 0:
            y.insert(0,float(j))
        else:
            x.insert(0,j)
        count += 1
    ax[i[0]].barh(x,y,height=0.5)
plt.tight_layout()





#Initialize the model
tfidf = gensim.models.TfidfModel(corpus)


corpus[30]


# apply transformation
tfidf[corpus[30]]


corpus_transformed = tfidf[corpus]


corpus_transformed[30]


tfidf.num_docs


ldamodel_tfidf = gensim.models.ldamodel.LdaModel(corpus_transformed, 
                                           num_topics=20, 
                                           id2word = dictionary, 
                                           passes=20)


for i in ldamodel_tfidf.print_topics(num_topics=20, num_words=20):
    print(i[0])
    print(i[1])
    print('\n')


fig,ax = plt.subplots(5,4,figsize=(15,20))
ax = ax.flatten()
for i in ldamodel_tfidf.print_topics(num_topics=20, num_words=20):
    x = []
    y = []
    count = 0
    for j in re.split(re.escape(' + ') + '|' + re.escape('*'), i[1]):
        if count % 2 == 0:
            y.insert(0,float(j))
        else:
            x.insert(0,j)
        count += 1
    ax[i[0]].barh(x,y,height=0.5)
plt.tight_layout()
